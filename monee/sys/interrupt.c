#include "interrupt.h"
#include <debug.h>
#include <inttypes.h>
#include <stdint.h>
#include <stdio.h>
#include <string.h>
#include "devices/timer.h"
#include "hw/i8259.h"
#include "mem/vaddr.h"
#include "peep/callouts.h"
#include "peep/tb.h"
#include "sys/flags.h"
#include "sys/gdt.h"
#include "sys/intr-stubs.h"
#include "sys/io.h"
#include "sys/loader.h"
#include "sys/mode.h"
#include "sys/monitor.h"
#include "sys/rr_log.h"
#include "sys/vcpu.h"
#include "threads/thread.h"
#include "threads/synch.h"

#define INT 2

/* Number of x86 interrupts. */
#define INTR_CNT 256

/* Programmable Interrupt Controller (PIC) registers.
 *    A PC has two PICs, called the master and slave PICs, with the
 *       slave attached ("cascaded") to the master IRQ line 2. */
#define PIC0_CTRL 0x20    /* Master PIC control register address. */
#define PIC0_DATA 0x21    /* Master PIC data register address. */
#define PIC1_CTRL 0xa0    /* Slave PIC control register address. */
#define PIC1_DATA 0xa1    /* Slave PIC data register address. */
#define IRQ_CASCADE0   2
#define IRQ_CASCADE1   9

/* The Interrupt Descriptor Table (IDT).  The format is fixed by
   the CPU.  See [IA32-v3a] sections 5.10 "Interrupt Descriptor
   Table (IDT)", 5.11 "IDT Descriptors", 5.12.1.2 "Flag Usage By
   Exception- or Interrupt-Handler Procedure". */
static uint64_t idt[INTR_CNT];
static uint64_t orig_idt[INTR_CNT];

/* Interrupt handler functions for each interrupt. */
static intr_handler_func *intr_handlers[INTR_CNT];

/* Names for each interrupt, for debugging purposes. */
static const char *intr_names[INTR_CNT];

/* cached values for PIC */
static uint8_t pic_mask[2];

/* External interrupts are those generated by devices outside the
   CPU, such as the timer.  External interrupts run with
   interrupts turned off, so they never nest, nor are they ever
   pre-empted.  Handlers for external interrupts also may not
   sleep, although they may invoke intr_yield_on_return() to
   request that a new process be scheduled just before the
   interrupt returns. */
static bool in_external_intr;   /* Are we processing an external interrupt? */
static bool yield_on_return;    /* Should we yield on interrupt return? */

/* Programmable Interrupt Controller helpers. */
static void init_pic(void);
static void pic_end_of_interrupt (int irq);

/* Interrupt Descriptor Table helpers. */
static uint64_t make_intr_gate (void (*) (void), int dpl);
static uint64_t make_trap_gate (void (*) (void), int dpl);
static inline uint64_t make_idtr_operand (uint16_t limit, void *base);
static void do_interrupt_protected(unsigned intno, int is_int, int error_code,
    uint32_t next_eip, int is_hw);
static void do_interrupt_real(unsigned intno, int is_int, int error_code,
    uint32_t next_eip);



/* Interrupt handlers. */
void intr_handler (struct intr_frame *args);

/* Returns the current interrupt status. */
enum intr_level
intr_get_level (void) 
{
  uint32_t flags;

  /* Push the flags register on the processor stack, then pop the
     value off the stack into `flags'.  See [IA32-v2b] "PUSHF"
     and "POP" and [IA32-v3a] 5.8.1 "Masking Maskable Hardware
     Interrupts". */
  asm volatile ("pushfl; popl %0" : "=g" (flags));

  return flags & IF_MASK ? INTR_ON : INTR_OFF;
}

/* Enables or disables interrupts as specified by LEVEL and
   returns the previous interrupt status. */
enum intr_level
intr_set_level (enum intr_level level) 
{
  return level == INTR_ON ? intr_enable () : intr_disable ();
}

/* Enables interrupts and returns the previous interrupt status. */
enum intr_level
intr_enable (void) 
{
  enum intr_level old_level = intr_get_level ();
  //enum mode_t mode = switch_to_kernel();
  ASSERT (!intr_context ());

  /* Enable interrupts by setting the interrupt flag.

     See [IA32-v2b] "STI" and [IA32-v3a] 5.8.1 "Masking Maskable
     Hardware Interrupts". */
  asm volatile ("sti");

  //switch_mode(mode);
  return old_level;
}

/* Disables interrupts and returns the previous interrupt status. */
enum intr_level
intr_disable (void) 
{
  enum intr_level old_level = intr_get_level ();
  //enum mode_t mode = switch_to_kernel();

  /* Disable interrupts by clearing the interrupt flag.
     See [IA32-v2b] "CLI" and [IA32-v3a] 5.8.1 "Masking Maskable
     Hardware Interrupts". */
  asm volatile ("cli" : : : "memory");

  //switch_mode(mode);
  return old_level;
}

/* Initializes the interrupt system.
 *
 * Translates the interrupt descriptor table (IDT) from real mode to
 * protected mode. In real mode, the IDT is stored at physical address
 * 0000:03ff. In protected mode, the idt is pointed to by the IDTR register.
 */
void
intr_init (void)
{
  uint64_t idtr_operand;
  uint32_t eflags;
  int i;

  /* Initialize interrupt controller. */
  init_pic ();

  memset(orig_idt, 0x0, sizeof orig_idt);

  /* Initialize orig IDT. */
  for (i = 0; i < INTR_CNT; i++) {
    /* Get the offset and and segment selector from the real-mode IDT stored
     * at [0000:03ff]
     */
    uint16_t off = *((uint16_t *)(i * 4));
    uint16_t seg = *((uint16_t *)(i*4 + 2));

    /* In real mode, the destination address is computed using
     * [(seg << 4) + off].
     */
    uintptr_t addr = (seg << 4) + off;
    if (addr != 0) {
      orig_idt[i] = make_intr_gate ((void *)addr, 0);
    }
  }

  /* Initialize IDT. */
  for (i = 0; i < INTR_CNT; i++) {
    idt[i] = make_intr_gate(intr_stubs[i], 0);
  }

  /* Load IDT register.
     See [IA32-v2a] "LIDT" and [IA32-v3a] 5.10 "Interrupt
     Descriptor Table (IDT)". */
  idtr_operand = make_idtr_operand (sizeof idt - 1, idt);
  asm volatile ("lidt %0" : : "m" (idtr_operand));

  /* Initialize intr_names. */
  for (i = 0; i < INTR_CNT; i++) {
    intr_names[i] = "unknown";
  }
  intr_names[0] = "#DE Divide Error";
  intr_names[1] = "#DB Debug Exception";
  intr_names[2] = "NMI Interrupt";
  intr_names[3] = "#BP Breakpoint Exception";
  intr_names[4] = "#OF Overflow Exception";
  intr_names[5] = "#BR BOUND Range Exceeded Exception";
  intr_names[6] = "#UD Invalid Opcode Exception";
  intr_names[7] = "#NM Device Not Available Exception";
  intr_names[8] = "#DF Double Fault Exception";
  intr_names[9] = "Coprocessor Segment Overrun";
  intr_names[10] = "#TS Invalid TSS Exception";
  intr_names[11] = "#NP Segment Not Present";
  intr_names[12] = "#SS Stack Fault Exception";
  intr_names[13] = "#GP General Protection Exception";
  intr_names[14] = "#PF Page-Fault Exception";
  intr_names[16] = "#MF x87 FPU Floating-Point Error";
  intr_names[17] = "#AC Alignment Check Exception";
  intr_names[18] = "#MC Machine-Check Exception";
  intr_names[19] = "#XF SIMD Floating-Point Exception";

  /* Set IOPL = 3, so that interrupts can be enabled and disabled in user
   * mode.
   */
  asm volatile ("pushfl; popl %0" : "=g"(eflags));
  eflags |= IOPL_MASK;
  asm volatile ("pushl %0; popfl" : : "g"(eflags));
}

/* Registers interrupt VEC_NO to invoke HANDLER with descriptor
   privilege level DPL.  Names the interrupt NAME for debugging
   purposes.  The interrupt handler will be invoked with
   interrupt status set to LEVEL. */
static void
register_handler (uint8_t vec_no, int dpl, enum intr_level level,
                  intr_handler_func *handler, const char *name)
{
  ASSERT (intr_handlers[vec_no] == NULL);
  if (level == INTR_ON)
    idt[vec_no] = make_trap_gate (intr_stubs[vec_no], dpl);
  else
    idt[vec_no] = make_intr_gate (intr_stubs[vec_no], dpl);
  intr_handlers[vec_no] = handler;
  intr_names[vec_no] = name;
}

void
unregister_handler (uint8_t vec_no)
{
  intr_handlers[vec_no] = NULL;
  intr_names[vec_no] = "unknown";
}

bool
intr_is_registered(uint8_t vec_no)
{
  return intr_handlers[vec_no] != NULL;
}

/* Registers external interrupt VEC_NO to invoke HANDLER, which
   is named NAME for debugging purposes.  The handler will
   execute with interrupts disabled. */
void
intr_register_ext (uint8_t vec_no, intr_handler_func *handler,
                   const char *name) 
{
  ASSERT (vec_no >= 0x20 && vec_no <= 0x2f);
  MSG ("%s(): Registering handler %p for IRQ 0x%x.\n", __func__, handler,
      vec_no);
  register_handler (vec_no, 0, INTR_OFF, handler, name);
}

/* Registers internal interrupt VEC_NO to invoke HANDLER, which
   is named NAME for debugging purposes.  The interrupt handler
   will be invoked with interrupt status LEVEL.

   The handler will have descriptor privilege level DPL, meaning
   that it can be invoked intentionally when the processor is in
   the DPL or lower-numbered ring.  In practice, DPL==3 allows
   user mode to invoke the interrupts and DPL==0 prevents such
   invocation.  Only "INT n, INT 3, or INTO" instructions are checked
   for privilege violation. All hardware-generated interrupts and
   processor-detected exceptions ignore the DPL of interrupt/trap gates.
   Hence, Faults and exceptions that occur in user mode
   still cause interrupts with DPL==0 to be invoked.  See
   [IA32-v3a] sections 5.12.1.1 "Protection of Exception- and Interrupt-Handler
   Procedures", 4.5 "Privilege Levels", 4.8.1.1
   "Accessing Nonconforming Code Segments" for further
   discussion. */
void
intr_register_int (uint8_t vec_no, int dpl, enum intr_level level,
                   intr_handler_func *handler, const char *name)
{
  ASSERT (vec_no < 0x20 || vec_no > 0x2f);
  register_handler (vec_no, dpl, level, handler, name);
}

/* Returns true during processing of an external interrupt
   and false at all other times. */
bool
intr_context (void) 
{
  return in_external_intr;
}

/* During processing of an external interrupt, directs the
   interrupt handler to yield to a new process just before
   returning from the interrupt.  May not be called at any other
   time. */
void
intr_yield_on_return (void) 
{
  ASSERT (intr_context ());
  yield_on_return = true;
}

/* 8259A Programmable Interrupt Controller. */

/* Initializes the PICs.  Refer to [8259A] for details.

   By default, interrupts 0...15 delivered by the PICs will go to
   interrupt vectors 0...15.  Those vectors are also used for CPU
   traps and exceptions, so we reprogram the PICs so that
   interrupts 0...15 are delivered to interrupt vectors 32...47
   (0x20...0x2f) instead. */
static void
init_pic(void)
{
  /* Mask all interrupts on both PICs. */
  outb (PIC0_DATA, 0xff);
  outb (PIC1_DATA, 0xff);

  /* Initialize master. */
  outb (PIC0_CTRL, 0x11); /* ICW1: single mode, edge triggered, expect ICW4. */
  outb (PIC0_DATA, 0x20); /* ICW2: line IR0...7 -> irq 0x20...0x27. */
  outb (PIC0_DATA, 0x04); /* ICW3: slave PIC on line IR2. */
  outb (PIC0_DATA, 0x01); /* ICW4: 8086 mode, normal EOI, non-buffered. */

  /* Initialize slave. */
  outb (PIC1_CTRL, 0x11); /* ICW1: single mode, edge triggered, expect ICW4. */
  outb (PIC1_DATA, 0x28); /* ICW2: line IR0...7 -> irq 0x28...0x2f. */
  outb (PIC1_DATA, 0x02); /* ICW3: slave ID is 2. */
  outb (PIC1_DATA, 0x01); /* ICW4: 8086 mode, normal EOI, non-buffered. */

  /* Unmask all interrupts. */
  outb (PIC0_DATA, 0x00);
  outb (PIC1_DATA, 0x00);
  pic_mask[0] = 0;
  pic_mask[1] = 0;
}

/* Sends an end-of-interrupt signal to the PIC for the given IRQ.
   If we don't acknowledge the IRQ, it will never be delivered to
   us again, so this is important.  */
static void
pic_end_of_interrupt (int irq) 
{
  ASSERT (irq >= 0x20 && irq < 0x30);

  /* Acknowledge master PIC. */
  outb (0x20, 0x20);

  /* Acknowledge slave PIC if this is a slave interrupt. */
  if (irq >= 0x28)
    outb (0xa0, 0x20);
}

/* Creates an gate that invokes FUNCTION.

   The gate has descriptor privilege level DPL, meaning that it
   can be invoked intentionally when the processor is in the DPL
   or lower-numbered ring.  In practice, DPL==3 allows user mode
   to call into the gate and DPL==0 prevents such calls.  Faults
   and exceptions that occur in user mode still cause gates with
   DPL==0 to be invoked.  See [IA32-v3a] sections 4.5 "Privilege
   Levels" and 4.8.1.1 "Accessing Nonconforming Code Segments"
   for further discussion.

   TYPE must be either 14 (for an interrupt gate) or 15 (for a
   trap gate).  The difference is that entering an interrupt gate
   disables interrupts, but entering a trap gate does not.  See
   [IA32-v3a] section 5.12.1.2 "Flag Usage By Exception- or
   Interrupt-Handler Procedure" for discussion. */
static uint64_t
make_gate (void (*function) (void), int dpl, int type)
{
  uint32_t e0, e1;

  ASSERT (function != NULL);
  ASSERT (dpl >= 0 && dpl <= 3);
  ASSERT (type >= 0 && type <= 15);

  e0 = (((uint32_t) function & 0xffff)     /* Offset 15:0. */
        | (SEL_KCSEG << 16));              /* Target code segment. */

  e1 = (((uint32_t) function & 0xffff0000) /* Offset 31:16. */
        | (1 << 15)                        /* Present. */
        | ((uint32_t) dpl << 13)           /* Descriptor privilege level. */
        | (0 << 12)                        /* System. */
        | ((uint32_t) type << 8));         /* Gate type. */

  return e0 | ((uint64_t) e1 << 32);
}

/* Creates an interrupt gate that invokes FUNCTION with the given
   DPL. */
static uint64_t
make_intr_gate (void (*function) (void), int dpl)
{
  return make_gate (function, dpl, 14);
}

/* Reads the FUNCTION pointer and the DPL of an interrupt gate. */
void
read_orig_intr_gate (int intno, void (**function)(void), int *dpl)
{
  uint32_t e0, e1;
  uint64_t gate;
  
  ASSERT(intno >= 0 && intno < INTR_CNT);
  gate = orig_idt[intno];

  e0 = gate & 0xffffffff;
  e1 = (gate >> 32);

  *function = (void *)((e0 & 0xffff) | (e1 & 0xffff0000));
  printf("e0=%#x, e1=%#x, *function=%p\n", e0, e1, *function);
  *dpl = (e1 >> 13) & 0x3;
}

/* Creates a trap gate that invokes FUNCTION with the given
   DPL. */
static uint64_t
make_trap_gate (void (*function) (void), int dpl)
{
  return make_gate (function, dpl, 15);
}

/* Returns a descriptor that yields the given LIMIT and BASE when
   used as an operand for the LIDT instruction. */
static inline uint64_t
make_idtr_operand (uint16_t limit, void *base)
{
  return limit | ((uint64_t) (uint32_t) base << 16);
}

/* Interrupt handlers. */

/* Handler for all interrupts, faults, and exceptions.  This
   function is called by the assembly language interrupt stubs in
   intr-stubs.S.  FRAME describes the interrupt and the
   interrupted thread's registers. */
static void
__intr_handler (struct intr_frame *frame)
{
  bool external;
  intr_handler_func *handler;

  /* External interrupts are special.
     We only handle one at a time (so interrupts must be off)
     and they need to be acknowledged on the PIC (see below).
     An external interrupt handler cannot sleep. */
  external = frame->vec_no >= 0x20 && frame->vec_no < 0x30;
  if (external) {
    ASSERT (intr_get_level () == INTR_OFF);
    ASSERT (!intr_context ());

    in_external_intr = true;
    yield_on_return = false;
  }

  /* Invoke the interrupt's handler. */
  handler = intr_handlers[frame->vec_no];
  if (handler != NULL) {
    handler (frame);
  } else if (!guest_intr_handler(frame)) {
    if (frame->vec_no == 0x27 || frame->vec_no == 0x2f) {
      /* There is no handler, but this interrupt can trigger
         spuriously due to a hardware fault or hardware race
         condition.  Ignore it. */
    } else {
      /* No handler and not spurious.  Invoke the unexpected
         interrupt handler. */
      intr_dump_frame (frame);
      PANIC ("Unexpected interrupt"); 
    }
  }

  /* Complete the processing of an external interrupt. */
  if (external) {
    ASSERT (intr_get_level () == INTR_OFF);
    ASSERT (intr_context ());

    in_external_intr = false;
    pic_end_of_interrupt (frame->vec_no); 

    if (yield_on_return) {
      thread_yield (); 
    }
	}
}

/* Sets up the stack pointer correctly, to avoid overwriting the monitor stack.
 * No stack-allocated local variables should be declared in this function.
 * There should be no function calls from this function.
 */
void
intr_handler (struct intr_frame *frame) 
{
  if ((frame->cs & 3) == 3) {
    static target_ulong ebp;
    /* Save the frame pointer. */
    asm volatile ("movl %%ebp, %0" : "=m"(ebp) : : "memory");

    /* Set the stack frame such that function calls see the correct stack. */
    asm volatile("movl %0, %%esp ; pushl %1 ; pushl %2 ; pushl %3 ; "
				"movl %%esp, %%ebp" ::
				"g"((frame->ss == SEL_UDSEG)?
															    frame->esp:(void *)last_monitor_context->esp),
        "g"(frame), "g"(*(uint32_t *)(ebp + 4)), "g"(*(uint32_t *)ebp));

    /* Ring 3 should only be used on the initial thread. */
    ASSERT(thread_current() == thread_initial());

    /* restore frame pointer so that function return sees the correct stack. */
    asm volatile ("movl %0, %%ebp" : : "m"(ebp) : "memory");
  }
  asm volatile ("pushl %0 ; call *%1; addl $4, %%esp ; leave ; ret" : :
      "m"(frame), "r"(__intr_handler));
	NOT_REACHED();
}

/* Dumps interrupt frame F to the console, for debugging. */
void
intr_dump_frame (const struct intr_frame *f) 
{
  uint32_t cr2;

  /* Store current value of CR2 into `cr2'.
     CR2 is the linear address of the last page fault.
     See [IA32-v2a] "MOV--Move to/from Control Registers" and
     [IA32-v3a] 5.14 "Interrupt 14--Page Fault Exception
     (#PF)". */
  asm ("movl %%cr2, %0" : "=r" (cr2));

  printf ("Interrupt %#04x (%s) at eip=%p [frame %p]\n",
          f->vec_no, intr_names[f->vec_no], f->eip, f);
  printf (" cr2=%08"PRIx32" error=%08"PRIx32"\n", cr2, f->error_code);
  printf (" eax=%08"PRIx32" ebx=%08"PRIx32" ecx=%08"PRIx32" edx=%08"
      PRIx32"\n", f->eax, f->ebx, f->ecx, f->edx);
  printf (" esi=%08"PRIx32" edi=%08"PRIx32" esp=%08"PRIx32" ebp=%08"
      PRIx32"\n", f->esi, f->edi, (uint32_t) f->esp, f->ebp);
  printf (" cs=%04"PRIx16" ds=%04"PRIx16" es=%04"PRIx16" ss=%04"PRIx16"\n",
          f->cs, f->ds, f->es, f->ss);

  printf("vcpu state:\n");
  printf(" cr0=%08"PRIx32" cr1=%08"PRIx32" cr2=%08"PRIx32"\n", vcpu.cr[0],
      vcpu.cr[1], vcpu.cr[2]);
  printf(" cr3=%08"PRIx32" cr4=%08"PRIx32"\n", vcpu.cr[3], vcpu.cr[4]);
  printf (" eax=%08"PRIx32" ebx=%08"PRIx32" ecx=%08"PRIx32" edx=%08"
      PRIx32"\n", vcpu.regs[R_EAX], vcpu.regs[R_EBX], vcpu.regs[R_ECX],
      vcpu.regs[R_EDX]);
  printf (" esi=%08"PRIx32" edi=%08"PRIx32" esp=%08"PRIx32" ebp=%08"
      PRIx32"\n", vcpu.regs[R_ESI], vcpu.regs[R_EDI], vcpu.regs[R_ESP],
      vcpu.regs[R_EBP]);
  printf (" cs=%04"PRIx16" ds=%04"PRIx16" es=%04"PRIx16" ss=%04"PRIx16"\n",
          vcpu.orig_segs[R_CS], vcpu.orig_segs[R_DS], vcpu.orig_segs[R_ES],
          vcpu.orig_segs[R_SS]);
  printf (" shadow:\n");
  printf ("   cs=%04"PRIx16" ds=%04"PRIx16" es=%04"PRIx16" ss=%04"PRIx16"\n",
          vcpu.segs[R_CS].selector, vcpu.segs[R_DS].selector,
          vcpu.segs[R_ES].selector, vcpu.segs[R_SS].selector);
}

/* Returns the name of interrupt VEC. */
const char *
intr_name (uint8_t vec) 
{
  return intr_names[vec];
}


/** masks a given IRQ */
void intr_irq_mask(int irq)
{
  if (irq < 8) {
    pic_mask[0] |= 1 << irq;
    outb (PIC0_DATA, pic_mask[0]);
  } else {
    pic_mask[1] |= 1 << (irq - 8);
    outb (PIC1_DATA, pic_mask[1]);
  }
}

/** unmasks a given IRQ */
void intr_irq_unmask(int irq)
{
  if (irq >= 8) {
    /* enable cascade if not enabled for pic2 */
    if(pic_mask[1] & (1 << (IRQ_CASCADE1 - 8))) {
      pic_mask[1] &= ~(1 << (IRQ_CASCADE1 - 8));
    }
    pic_mask[1] &= ~(1 << (irq - 8));
    outb(PIC1_DATA, pic_mask[1]);

    /* enable cascade if not enabled for pic1 */
    if(pic_mask[0] & (1 << IRQ_CASCADE0)) {
      irq = IRQ_CASCADE0;
    }
  }

  if(irq < 8) {
    pic_mask[0] &= ~(1 << irq);
    outb (PIC0_DATA, pic_mask[0]);
  }
}

void
do_interrupt(unsigned intno, int is_int, int error_code, uint32_t next_eip,
    int is_hw)
{
  if (vcpu.cr[0] & CR0_PE_MASK) {
    do_interrupt_protected(intno, is_int, error_code, next_eip, is_hw);
  } else {
    do_interrupt_real(intno, is_int, error_code, next_eip);
  }
}

static inline void get_ss_esp_from_tss(uint32_t *ss_ptr,
                                           uint32_t *esp_ptr, int dpl)
{
  unsigned type, index, shift;

  if (!(vcpu.tr.flags & DESC_P_MASK)) {
    NOT_IMPLEMENTED();
    //cpu_abort(env, "invalid tss");
  }
  type = (vcpu.tr.flags >> DESC_TYPE_SHIFT) & 0xf;
  if ((type & 7) != 1) {
    NOT_IMPLEMENTED();
    //cpu_abort(env, "invalid tss type");
  }
  shift = type >> 3;
  index = (dpl * 4 + 2) << shift;
  if (index + (4 << shift) - 1 > vcpu.tr.limit) {
    raise_exception_err(EXCP0A_TSS, vcpu.tr.selector & 0xfffc);
  }
  if (shift == 0) {
    *esp_ptr = lduw_kernel(vcpu.tr.base + index);
    *ss_ptr = lduw_kernel(vcpu.tr.base + index + 2);
  } else {
    *esp_ptr = ldl_kernel(vcpu.tr.base + index);
    *ss_ptr = lduw_kernel(vcpu.tr.base + index + 4);
  }
}

static void
do_interrupt_protected(unsigned intno, int is_int, int error_code,
		uint32_t next_eip, int is_hw)
{
  desc_table_t *dt;
  target_ulong ptr, ssp;
	unsigned type, dpl, selector, ss_dpl, cpl;
  int has_error_code, new_stack, shift;
  uint32_t e1, e2, offset, ss, esp, ss_e1, ss_e2;
  uint32_t old_eip, sp_mask, push_eflags;

  segcache_sync(R_SS);

  has_error_code = 0;
  if (!is_int && !is_hw) {
    switch(intno) {
      case 8:
      case 10:
      case 11:
      case 12:
      case 13:
      case 14:
      case 17:
        has_error_code = 1;
        break;
    }
  }
  if (is_int) {
    old_eip = next_eip;
  } else {
    old_eip = (uint32_t)vcpu.eip;
  }

  dt = &vcpu.idt;
  if (intno * 8 + 7 > dt->limit) {
    raise_exception_err(EXCP0D_GPF, intno * 8 + 2);
  }
  ptr = dt->base + intno * 8;
  e1 = ldl_kernel(ptr);
  e2 = ldl_kernel(ptr + 4);
  /* check gate type */
  type = (e2 >> DESC_TYPE_SHIFT) & 0x1f;
  switch(type) {
    case 5: /* task gate */
      NOT_IMPLEMENTED();
#if 0
      /* must do that check here to return the correct error code */
      if (!(e2 & DESC_P_MASK)) {
        raise_exception_err(EXCP0B_NOSEG, intno * 8 + 2);
      }
      switch_tss(intno * 8, e1, e2, SWITCH_TSS_CALL, old_eip);
      if (has_error_code) {
        int type;
        uint32_t mask;
        /* push the error code */
        type = (env->tr.flags >> DESC_TYPE_SHIFT) & 0xf;
        shift = type >> 3;
        if (env->segs[R_SS].flags & DESC_B_MASK)
          mask = 0xffffffff;
        else
          mask = 0xffff;
        esp = (ESP - (2 << shift)) & mask;
        ssp = env->segs[R_SS].base + esp;
        if (shift)
          stl_kernel(ssp, error_code);
        else
          stw_kernel(ssp, error_code);
        SET_ESP(esp, mask);
      }
      return;
#endif
    case 6: /* 286 interrupt gate */
    case 7: /* 286 trap gate */
    case 14: /* 386 interrupt gate */
    case 15: /* 386 trap gate */
      break;
    default:
      printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
      raise_exception_err(EXCP0D_GPF, intno * 8 + 2);
      break;
  }
  dpl = (e2 >> DESC_DPL_SHIFT) & 3;
  cpl = vcpu.orig_segs[R_CS] & 3;
  //LOG(INT, "%s() %d: dpl=%d, cpl=%d\n", __func__, __LINE__, dpl, cpl);
  /* check privledge if software int */
  if (is_int && dpl < cpl) {
    printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, intno * 8 + 2);
  }
  /* check valid bit */
  if (!(e2 & DESC_P_MASK)) {
    printf("%s() %d: raising #NOSEG\n", __func__, __LINE__);
    raise_exception_err(EXCP0B_NOSEG, intno * 8 + 2);
  }
  selector = e1 >> 16;
  offset = (e2 & 0xffff0000) | (e1 & 0x0000ffff);
  if ((selector & 0xfffc) == 0) {
    printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, 0);
  }

  if (!read_segment(&e1, &e2, selector, false, true)) {
    printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, selector & 0xfffc);
  }
  if (!(e2 & DESC_S_MASK) || !(e2 & (DESC_CS_MASK))) {
    printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, selector & 0xfffc);
  }
  dpl = (e2 >> DESC_DPL_SHIFT) & 3;
  if (dpl > cpl) {
    printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, selector & 0xfffc);
  }
  if (!(e2 & DESC_P_MASK)) {
    printf("%s() %d: raising #NOSEG\n", __func__, __LINE__);
    raise_exception_err(EXCP0B_NOSEG, selector & 0xfffc);
  }
  if (!(e2 & DESC_C_MASK) && dpl < cpl) {
    /* to inner priviledge */
    get_ss_esp_from_tss(&ss, &esp, dpl);
		//LOG(INT, "Interrupt to inner privilege. ss=0x%x, esp=0x%x\n", ss, esp);
    if ((ss & 0xfffc) == 0) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    if ((ss & 3) != dpl) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    if (!read_segment(&ss_e1, &ss_e2, ss, false, true)) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    ss_dpl = (ss_e2 >> DESC_DPL_SHIFT) & 3;
    if (ss_dpl != dpl) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    if (!(ss_e2 & DESC_S_MASK) ||
        (ss_e2 & DESC_CS_MASK) ||
        !(ss_e2 & DESC_W_MASK)) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    if (!(ss_e2 & DESC_P_MASK)) {
      //printf("Calling raise_exception_err(0x%x).\n", EXCP0A_TSS);
      raise_exception_err(EXCP0A_TSS, ss & 0xfffc);
    }
    new_stack = 1;
    sp_mask = get_sp_mask(ss_e2);
    ssp = get_seg_base(ss_e1, ss_e2);
  } else if ((e2 & DESC_C_MASK) || dpl == cpl) {
    /* to same privilege */
    if (vcpu.eflags & VM_MASK) {
      printf("%s() %d: raising #GPF.\n", __func__, __LINE__);
      raise_exception_err(EXCP0D_GPF, selector & 0xfffc);
    }
    new_stack = 0;
    sp_mask = get_sp_mask(vcpu.segs[R_SS].flags);
    ssp = vcpu.segs[R_SS].base;
    esp = vcpu.regs[R_ESP];
    dpl = cpl;
  } else {
    printf("%s() %d: raising #GPF\n", __func__, __LINE__);
    raise_exception_err(EXCP0D_GPF, selector & 0xfffc);
    new_stack = 0; /* avoid warning */
    sp_mask = 0; /* avoid warning */
    ssp = 0; /* avoid warning */
    esp = 0; /* avoid warning */
  }

  shift = type >> 3;

	if (vcpu.IF == 2) {
		vcpu.IF = 1;
	}
	ASSERT(vcpu.IF == 0 || vcpu.IF == 1);
  /* XXX: need to check if enough room is available. see qemu code. */
  push_eflags = (vcpu.eflags & ~IF_MASK & ~IOPL_MASK) | ((vcpu.IF == 1)?IF_MASK:0)
		| ((uint32_t)vcpu.IOPL << 12) | (vcpu.AC?AC_MASK:0);
  if (shift == 1) {
    if (new_stack) {
      if (vcpu.eflags & VM_MASK) {
        PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_GS]);
        PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_FS]);
        PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_DS]);
        PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_ES]);
      }
      PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_SS]);
      PUSHL(ssp, esp, sp_mask, vcpu.regs[R_ESP]);
    }
    PUSHL(ssp, esp, sp_mask, push_eflags);
    PUSHL(ssp, esp, sp_mask, vcpu.orig_segs[R_CS]);
    PUSHL(ssp, esp, sp_mask, old_eip);
		//printf("%s(): pushed 0x%x at 0x%x\n", __func__, old_eip, esp);
    if (has_error_code) {
      PUSHL(ssp, esp, sp_mask, error_code);
    }
  } else {
    if (new_stack) {
      if (vcpu.eflags & VM_MASK) {
        PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_GS]);
        PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_FS]);
        PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_DS]);
        PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_ES]);
      }
      PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_SS]);
      PUSHW(ssp, esp, sp_mask, vcpu.regs[R_ESP]);
    }
    PUSHW(ssp, esp, sp_mask, push_eflags);
    PUSHW(ssp, esp, sp_mask, vcpu.orig_segs[R_CS]);
    PUSHW(ssp, esp, sp_mask, old_eip);
    if (has_error_code) {
      PUSHW(ssp, esp, sp_mask, error_code);
    }
  }

  if (new_stack) {
    if (vcpu.eflags & VM_MASK) {
      NOT_IMPLEMENTED();
      /*
      cpu_x86_load_seg_cache(env, R_ES, 0, 0, 0, 0);
      cpu_x86_load_seg_cache(env, R_DS, 0, 0, 0, 0);
      cpu_x86_load_seg_cache(env, R_FS, 0, 0, 0, 0);
      cpu_x86_load_seg_cache(env, R_GS, 0, 0, 0, 0);
      */
    }
    ss = (ss & ~3) | dpl;
    load_seg_cache(R_SS, ss, ssp, get_seg_limit(ss_e1, ss_e2), ss_e2);
  }
  SET_ESP(esp, sp_mask);

  selector = (selector & ~3) | dpl;
  load_seg_cache(R_CS, selector, 
      get_seg_base(e1, e2),
      get_seg_limit(e1, e2),
      e2);
  //cpu_x86_set_cpl(env, dpl);
  vcpu.eip = (void *)offset;
	//LOG(INT, "%s(): jumping to interrupt handler: %p\n", __func__, vcpu.eip);

  /* interrupt gate clear IF mask */
  if ((type & 1) == 0) {
    vcpu.IF = 0;
  }
  vcpu.eflags &= ~(TF_MASK | VM_MASK | RF_MASK | NT_MASK);
}

static void
do_interrupt_real(unsigned intno, int is_int, int error_code UNUSED,
		uint32_t next_eip)
{
  desc_table_t *dt;
  uint32_t ptr, ssp;
  uint32_t offset, esp;
  uint32_t old_cs, old_eip;
  int selector;

  segcache_sync(R_SS);
  /*printf("%s(%#x, %d, %#x, %#x) called.\n", __func__, intno, is_int,
      error_code, next_eip);*/
  /* real mode (simpler!) */
  dt = &vcpu.idt;
  if (intno*4 + 3 > dt->limit) {
    raise_exception_err(EXCP0D_GPF, intno*8 + 2);
  }
  ptr = dt->base + intno * 4;
  offset = lduw_kernel(ptr);
  selector = lduw_kernel(ptr + 2);
  esp = vcpu.regs[R_ESP];
  ssp = vcpu.segs[R_SS].base;
  if (is_int) {
    old_eip = next_eip;
  } else {
    old_eip = (uint32_t)vcpu.eip;
  }
  //printf("old_eip = %#x pushed to %#x\n", old_eip, esp);
  old_cs = vcpu.segs[R_CS].base >> 4;
	if (vcpu.IF == 2) {
		vcpu.IF = 1;
	}
	ASSERT(vcpu.IF == 0 || vcpu.IF == 1);
  PUSHW(ssp, esp, 0xffff, (vcpu.eflags & ~IF_MASK & ~IOPL_MASK)
      | ((vcpu.IF == 1)?IF_MASK:0) | ((uint32_t)vcpu.IOPL << 12)
      | (vcpu.AC?AC_MASK:0));
  PUSHW(ssp, esp, 0xffff, old_cs);
  PUSHW(ssp, esp, 0xffff, old_eip);

  /* update processor state. */
  vcpu.regs[R_ESP] = (vcpu.regs[R_ESP] & ~0xffff) | (esp & 0xffff);
  vcpu.eip = (void *)offset;
  load_seg_cache(R_CS, selector, selector << 4, 0xffff, 0);
  vcpu.eflags &= ~(TF_MASK | AC_MASK | RF_MASK);
  ASSERT((vcpu.eflags & AC_MASK) == 0);
  vcpu.AC = 0;
  vcpu.IF = 0;
}

void
raise_exception_err(int exception_index, int error_code)
{
  LOG(PCALL, "%s(): calling raise_interrupt(%#x)\n", __func__, exception_index);
  raise_interrupt(exception_index, 0, error_code, 0);
}

/* Signal an interruption. It is executed in the main CPU loop.
 * is_int is TRUE if coming from the int instruction. next_eip is the
 * EIP value AFTER the interrupt instruction. It is only relevant if
 * is_int is TRUE. error_code=-1 indicates, it is an external interrupt.
 */
void raise_interrupt(int intno, int is_int_insn, int error_code,
    int next_eip)
{
  ASSERT(read_cpl() == 3);
  vcpu.exception_index = intno;
  vcpu.error_code = error_code;
  vcpu.exception_is_int = is_int_insn;
  vcpu.exception_next_eip = next_eip;
  if (!is_int_insn && error_code == -1) {        /* external interrupt. */
    /* printf("%s(): calling rr_interrupt(). intno=%d, vcpu.n_exec=0x%llx\n",
        __func__, intno, vcpu.n_exec); */
    rr_interrupt(intno, error_code, next_eip);
  }
  /* printf("%s(): calling longjmp(). vcpu.n_exec=0x%llx\n", __func__,
      vcpu.n_exec); */
  longjmp(vcpu.jmp_env, 1);
}

static void
load_eflags(uint32_t new_eflags, uint32_t eflags_mask)
{
  eflags_mask &= ~(IF_MASK | IOPL_MASK | AC_MASK);
  vcpu.eflags = (vcpu.eflags & ~eflags_mask) | (new_eflags & eflags_mask);
}

void
iret_real(void)
{
  uint32_t sp, new_cs, new_eip, new_eflags, sp_mask, eflags_mask;
  target_ulong ssp;
  int shift = 0;    //XXX

  segcache_sync(R_SS);
  sp_mask = 0xffff;
  ssp = vcpu.segs[R_SS].base;
  sp = vcpu.regs[R_ESP];
  if (shift == 1) {
    /* 32 bits. */
    POPL(ssp, sp, sp_mask, new_eip);
    POPL(ssp, sp, sp_mask, new_cs);
    new_cs &= 0xffff;
    POPL(ssp, sp, sp_mask, new_eflags);
  } else {
    /* 16 bits. */
    POPW(ssp, sp, sp_mask, new_eip);
    POPW(ssp, sp, sp_mask, new_cs);
    POPW(ssp, sp, sp_mask, new_eflags);
  }
  vcpu.regs[R_ESP] = (vcpu.regs[R_ESP] & ~sp_mask) | (sp & sp_mask);
  new_cs &= 0xffff;
  load_seg_cache(R_CS, new_cs, new_cs << 4, 0xffff, 0);
  vcpu.eip = (void *)new_eip;
  eflags_mask = 0xffffffff;
  if (shift == 0) {
    eflags_mask &= 0xffff;
  }
  if (!(vcpu.eflags & VM_MASK)) {
    vcpu.IOPL = (new_eflags >> IOPL_SHIFT) & 3;
  }
  load_eflags(new_eflags, eflags_mask);
  vcpu.IF = (new_eflags & IF_MASK)?1:0;
  vcpu.AC = (new_eflags & AC_MASK)?1:0;
}

static inline void
validate_seg(int seg_reg, int cpl)
{
  int dpl;
  uint32_t e2;

  if ((seg_reg == R_FS || seg_reg == R_GS) &&
      (vcpu.orig_segs[seg_reg] & 0xfffc) == 0) {
    return;
  }
  segcache_sync(seg_reg);

  e2 = vcpu.segs[seg_reg].flags;
  dpl = (e2 >> DESC_DPL_SHIFT) & 3;
  if (!(e2 & DESC_CS_MASK) || !(e2 & DESC_C_MASK)) {
    /* data or non conforming code segment */
    if (dpl < cpl) {
      load_seg_cache(seg_reg, 0, 0, 0, 0);
    }
  }
}

static void
ret_protected(int is_iret, int addend)
{
  uint32_t new_cs, new_eflags, new_ss;
  //uint32_t new_es, new_ds, new_fs, new_gs;
  uint32_t e1, e2, ss_e1, ss_e2;
  unsigned cpl, dpl, rpl, iopl;
  target_ulong ssp, sp, new_eip, new_esp, sp_mask;
  int shift = 1; //XXX

  segcache_sync(R_SS);
  sp_mask = get_sp_mask(vcpu.segs[R_SS].flags);
  sp = vcpu.regs[R_ESP];
  ssp = vcpu.segs[R_SS].base;
  new_eflags = 0; /* avoid warning */
  if (shift == 1) {
    /* 32 bits */
    POPL(ssp, sp, sp_mask, new_eip);
    POPL(ssp, sp, sp_mask, new_cs);
    //printf("new_cs=0x%x\n", new_cs);
    new_cs &= 0xffff;
    if (is_iret) {
      POPL(ssp, sp, sp_mask, new_eflags);
      if (new_eflags & VM_MASK) {
        goto return_to_vm86;
      }
    }
  } else {
    /* 16 bits */
    POPW(ssp, sp, sp_mask, new_eip);
    POPW(ssp, sp, sp_mask, new_cs);
    if (is_iret) {
      POPW(ssp, sp, sp_mask, new_eflags);
    }
  }
  if ((new_cs & 0xfffc) == 0) {
    raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
  }
  if (!read_segment(&e1, &e2, new_cs, false, true)) {
    raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
  }
  if (!(e2 & DESC_S_MASK) ||
      !(e2 & DESC_CS_MASK)) {
    raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
  }
  cpl = vcpu.orig_segs[R_CS] & 3;
  rpl = new_cs & 3; 
  if (rpl < cpl) {
    raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
  }
  dpl = (e2 >> DESC_DPL_SHIFT) & 3;
  if (e2 & DESC_C_MASK) {
    if (dpl > rpl) {
      raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
    }
  } else {
    if (dpl != rpl) {
      raise_exception_err(EXCP0D_GPF, new_cs & 0xfffc);
    }
  }
  if (!(e2 & DESC_P_MASK)) {
    raise_exception_err(EXCP0B_NOSEG, new_cs & 0xfffc);
  }

  sp += addend;
  if (rpl == cpl) {
    /* return to same privilege level */
    load_seg_cache(R_CS, new_cs, 
        get_seg_base(e1, e2),
        get_seg_limit(e1, e2),
        e2);
  } else {
    /* return to different privilege level */
    if (shift == 1) {
      /* 32 bits */
      POPL(ssp, sp, sp_mask, new_esp);
      POPL(ssp, sp, sp_mask, new_ss);
      new_ss &= 0xffff;
    } else {
      /* 16 bits */
      POPW(ssp, sp, sp_mask, new_esp);
      POPW(ssp, sp, sp_mask, new_ss);
    }
    DBGn(INT, "%s(): vcpu.eip=%p, new_eip=%p, rpl=%d, "
        "cpl=%d\n", __func__, vcpu.eip, (void *)new_eip,
        rpl, cpl);
    if ((new_ss & 0xfffc) == 0) {
      raise_exception_err(EXCP0D_GPF, 0);
    } else {
      if ((new_ss & 3) != rpl) {
        raise_exception_err(EXCP0D_GPF, new_ss & 0xfffc);
      }
      if (!read_segment(&ss_e1, &ss_e2, new_ss, false, true)) {
        printf("%s() %d:\n", __func__, __LINE__);
        raise_exception_err(EXCP0D_GPF, new_ss & 0xfffc);
      }
      if (!(ss_e2 & DESC_S_MASK) ||
          (ss_e2 & DESC_CS_MASK) ||
          !(ss_e2 & DESC_W_MASK)) {
        raise_exception_err(EXCP0D_GPF, new_ss & 0xfffc);
      }
      dpl = (ss_e2 >> DESC_DPL_SHIFT) & 3;
      if (dpl != rpl) {
        raise_exception_err(EXCP0D_GPF, new_ss & 0xfffc);
      }
      if (!(ss_e2 & DESC_P_MASK)) {
        raise_exception_err(EXCP0B_NOSEG, new_ss & 0xfffc);
      }
      load_seg_cache(R_SS, new_ss, 
          get_seg_base(ss_e1, ss_e2),
          get_seg_limit(ss_e1, ss_e2),
          ss_e2);
    }

    load_seg_cache(R_CS, new_cs, 
        get_seg_base(e1, e2),
        get_seg_limit(e1, e2),
        e2);
    //cpu_x86_set_cpl(env, rpl);
    sp = new_esp;
    sp_mask = get_sp_mask(ss_e2);

    /* validate data segments */
    validate_seg(R_ES, rpl);
    validate_seg(R_DS, rpl);
    validate_seg(R_FS, rpl);
    validate_seg(R_GS, rpl);

    sp += addend;
  }
  SET_ESP(sp, sp_mask);

  vcpu.eip = (void *)new_eip;
	if ((vcpu.orig_segs[R_CS] & 3) == 0) {
		ASSERT((target_ulong)vcpu.eip >= 0xc0000000);
	}
	if ((vcpu.orig_segs[R_CS] & 3) == 3) {
		if ((target_ulong)vcpu.eip >= 0xc0000000) {
			printf("%s(): n_exec=%llx, vcpu.eip=%p\n", __func__, vcpu.n_exec, vcpu.eip);
		}
		ASSERT((target_ulong)vcpu.eip < 0xc0000000);
	}

  if (is_iret) {
    uint32_t eflags_mask;
    eflags_mask = 0;
    eflags_mask |= CC_C | CC_P | CC_A | CC_Z | CC_S | CC_O;
    /* NOTE: 'cpl' is the _old_ CPL */
    eflags_mask |= TF_MASK | ID_MASK | RF_MASK | NT_MASK;
    iopl = vcpu.IOPL;
    if (cpl <= iopl) {
      vcpu.IF = 1;
    }
    if (shift == 0) {
      eflags_mask &= 0xffff;
    }
    load_eflags(new_eflags, eflags_mask);
    if (cpl == 0) {
      vcpu.IOPL = (new_eflags >> IOPL_SHIFT) & 3;
      vcpu.AC = (new_eflags >> AC_SHIFT) & 1;
    }
  }

	//printf("%s(): exiting. eip=%p, esp=%x\n", __func__, vcpu.eip, vcpu.regs[R_ESP]);
  return;

return_to_vm86:
  NOT_IMPLEMENTED();
#if 0
  POPL(ssp, sp, sp_mask, new_esp);
  POPL(ssp, sp, sp_mask, new_ss);
  POPL(ssp, sp, sp_mask, new_es);
  POPL(ssp, sp, sp_mask, new_ds);
  POPL(ssp, sp, sp_mask, new_fs);
  POPL(ssp, sp, sp_mask, new_gs);

  /* modify processor state */
  load_eflags(new_eflags, TF_MASK | AC_MASK | ID_MASK | 
      IF_MASK | IOPL_MASK | VM_MASK | NT_MASK | VIF_MASK | VIP_MASK);
  load_seg_vm(R_CS, new_cs & 0xffff);
  cpu_x86_set_cpl(env, 3);
  load_seg_vm(R_SS, new_ss & 0xffff);
  load_seg_vm(R_ES, new_es & 0xffff);
  load_seg_vm(R_DS, new_ds & 0xffff);
  load_seg_vm(R_FS, new_fs & 0xffff);
  load_seg_vm(R_GS, new_gs & 0xffff);

  env->eip = new_eip & 0xffff;
  ESP = new_esp;
#endif
}

void
iret_protected(void)
{
  if (vcpu.eflags & NT_MASK) {
    NOT_IMPLEMENTED();
  } else {
    ret_protected(1, 0);
  }
}
